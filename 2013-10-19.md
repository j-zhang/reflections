2013-10-19

2013-10-16

I had a good working session on Wednesday trying to get started on the assignment. After the helpful session last class when we tried to reproduce our classmates' projects, I had learned how to walk through iPython Notebook, but was missing access to some of the packages they were using. This time around, I followed the instructions from the assignment page, and installed new packages. I remembered to clone the repository so I could access the iPython Notebook when I open up in the browser
```git
git clone https://github.com/j-zhang/recent-quakes
```
When installing pandas, I at first tried the command, 
```python
pip install pandas
```
but this failed to work when I ran the example Notebook. I explored a few websites, including http://pandas.pydata.org/pandas-docs/stable/install.html and http://nbviewer.ipython.org/, and instead used the command
```python
sudo apt-get install python-pandas
```
After successfully re-running the Notebook that Aaron demonstrated in class, I explored using JSON as a data source. Some of the issues I ran into are documented here: https://github.com/stat157/recent-quakes/issues/3. While I'm currently still stuck on the issue, working through all this and searching online and using resources like the iPython Notebook website, Python documentation, and StackOverflow for JSON problems, I definitely learned a lot and am more comfortable working in iPython Notebook and have more familiarity with Python.

2013-10-19

Today was another rough morning and afternoon trying to understand the assignment better. There a significant amount of frustrations earlier on in the day, but most of them have been resolved in these last few hours working through the assignment. This morning when we attended office hours, there was already another group there, but we hadn't seen that on the bCal that we usually sign up on. While working together was fine, we had the challenge of trying to ask the questions we needed to. Our schedule 11-12:30 pm office hours were cut short half way through because of a conflicting appointment. The situation was very frustrating because we had committed our morning and noontime to this assignment, yet our time was not respected. Compared to office hours the previous week, I didn't feel like we had made much progress. I didn't feel like we were provided much guidance and support, and instead were told to go into another room and keep figuring it out. We were told to use the Python for Data Analysis book as a resource, but given that in the past, what we've been told to read on our own before class we've never followed up on, it was tough just being handed the book. Also, it was unnerving that the instructions were changing, in the sense that at one point we didn't have to us the JSON format to do our assignment, and just needed to focus on the cache part. Granted, the focus was on being defensible in our decisions. After another three hours on a Saturday, I came out of the session frustrated and confused.

After eating some lunch, I decided to tackle the assignment on my own, and was keen on accomplishing the task and goal of being able to parse through the data in JSON format. One important thing was understanding how data is formatted in JSON, as it consists of lists and dictionaries, and one significant challenge was understanding how to access the data that I wanted.

The <a href="http://pandas.pydata.org/pandas-docs/stable/10min.html">pandas documentation</a> was critical in understanding how to select data, view the data frame, append data, etc. Then, there was also needing to figure out how the <a href="http://earthquake.usgs.gov/earthquakes/catalogs/eqs7day-M1.txt">old data feed</a> differed from the <a href="http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/1.0_week.geojson">new data feed</a>, and figuring out where the variables we needed were stored. One strategy I used was writing the code in Python for one earthquake, and then getting all the information for all the earthquakes of magnitude 1.0 for the last week.

After completing those steps by reading through a significant amount of documentation, I decided to explore the visualization, to see if the data cleaned and curated from JSON format would work. There was a very helpful <a href="http://peak5390.wordpress.com/2012/12/08/matplotlib-basemap-tutorial-plotting-points-on-a-simple-map/">Basemap tutorial</a> that I walked through to understand how to change the view from Alaska to California. To get latitudes and longitudes, I used the site: http://itouchmap.com/latlong.html.

The final thing I worked on this afternoon was understanding how to ```git push```. It required a process of ```git add```, ```git commit```, and ```git push``` commands. I also had to understand how the various repositories were organized in the VM and on GitHub, as well as understand how forking played a role. I was then successfully able to upload a draft of the iPython Notebook into <a href="https://github.com/j-zhang/recent-quakes">my repository</a>.

After an afternoon of working through this, I've felt I have a better understanding of git, Python, JSON, and visualization through the Basemap tool from matplotlib. Talking to my peers about the assignment, and supporting one another was also very helpful in getting through much of the assignment thus far. Also getting pointers from my friends outside the class was extremely helpful.

2013-10-21

In summary:

####Roadblocks:

#####Data Curation:

+ Unfamiliar with JSON representation, and navigating and accessing keys, values, and items in the lists and dictionaries
+ Lack of working knowledge in Python, and understanding how syntax differs from R (i.e. indentation, for loops), as well as how importing libraries and using functions worked
+ How to ```git push``` .ipynb and .py files to the working repository to share code and collaborate with group

Problem solving was a key part to understanding how to work through the data curation stages. JSON was a completely new format that we were unfamiliar with, coupled with learning Python and working in the iPython Notebook (which our group had familiarized ourselves with after the first assignment). We used numerous **strategies** to tackle this assignment:

Read up on documentation:

+ <a href="http://pandas.pydata.org/pandas-docs/stable/10min.html">Pandas documentation</a> helped to understand how to select data, view the data frame, append data, etc.
+ <a href="http://earthquake.usgs.gov/earthquakes/catalogs/eqs7day-M1.txt">Old data feed</a> differed from the <a href="http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/1.0_week.geojson">new data feed</a>, so searching for relationships between variables of the data helped allow more familiarity with how lists and dictionaries are organized.
+ Another strategy was working through extracting the data for the variables of one single earthquake, then applying the for loop to all earthquakes and then generating the new data frame from <a href="http://docs.scipy.org/doc/numpy/reference/arrays.html">arrays</a>, reading up on ```numpy```

Work with others and ask friends:

+ Asked for explanation from EECS friends more familiar with GitHub on concept of ```git push``` since didn't understand how to push .ipynb and .py files from Virtual Machine; learned that it involves a sequence of commands
