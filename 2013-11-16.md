2013-11-16

2013-11-14 

This week has been one of those slower weeks. I was diappointed that we didn't have speakers this week, since I was really looking forward to the guests present. In both class periods, the class ended up being more of a working session for the groups. Although there wasn't much structure, ultimately I think it was good that we had some time to discuss with our larger horizontal group, as opposed to simply meeting up in our smaller analyzer subgroup. On Wednesday, my subgroup met in the evening for a number of hours to try and "regroup." In the past few classes, we've been trying to better understand how the optimization problem worked, or figuring out how to improve upon the alarm strategy. It felt like we were stagnating and re-tackling the same issue class after class. The first hour we met, we worked to reproduce some of the visualizers' code on error diagrams. As we ran through the code, we found it very difficult to reproduce, running into issues of confusing variable names, not knowing what the data source was, etc. Working through this reproducibility exercise really reinforced the importance of writing reproducible code. After posting on their issue tracker, we then realized that they had modified some of Luen's code, which in retrospect we should have explored first. Nonetheless, it was a helpful exercise and reminder for when we continue working. Afterwards we tried to tackle how to evaluate the function that we would improve, but that continued our same problem of not fully knowing how to improve the model. Ultimately, we spent our last hour hashing out a SMART goal, and come up with a plan of something that we could attain given the time and knowledge constraints. Overall, the three hours that we met were really a sum of a lot of the concepts that we've learned over the semester. Having hashed out a new plan, we're ready to start tackling our problem again this weekend.

2013-11-16

This afternoon, we talked about writing skeleton code to plot aftershock arrival times. There were many considerations this afternoon when starting to hash out the problem. We had to consider which catalogs to use, since most earthquakes are of smaller magnitudes. For much of the early afternoon, we emphasized and were too engrossed in using ```ggplot```, that we tried to plot histograms of the magnitude data for us, but didn't have a deep understanding of how to use the package. After getting back on track, we decided to use the 2012 earthquake catalog. We also had lengthy discussions on how we wanted to store the data, as lists of tuples, lists of lists, in a combined data frame with magnitudes and time deltas, as separate variables storing each, etc. We chose an arbitrary five aftershocks to plot for each earthquake. We also had to dig into binning, and how we could do that using ```ggplot```. In order to do some analysis on the bins, we forewent ```ggplot``` for the time being and used a function in ```numpy```. 
